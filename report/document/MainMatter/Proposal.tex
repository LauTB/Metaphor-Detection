\chapter{Marco teórico}\label{chapter:proposal}
\section{Representación del texto}
Cuando se va a trabajar con texto es necesario encontrar una representación adecuada del mismo. Existen diversas formas de representar texto, entre ellas dos de las más populares son las bolsas de palabras y los word embeddings. \\
Las bolsas de palabras (bag of words o representación one-shot) tienen un vector binario del tamaño del vocabulario donde cada componente representa un término. La ventaja principal de es la simplicidad de representación. La mayor desventaja se encuentran el gran tamaño de los vectores a los cuales tendrán la mayoría de las componentes nulas; la otra desventaja es que se pierde el orden de las palabras.\\
Por otra parte, los word embeddings representan cada palabra como un vector donde cada componente tiene un valor numérico, la dimensión es variable. Una propiedad Que posee dicha representación es que si dos Palabras son semánticamente similares entonces los vectores que los representan serán cercanos.\\
Existen algoritmos diversos para crear dichos word embeddings y en su mayoría pasan por técnicas de Aprendizaje de Maquinas. En esta investigación se utilizaron word embeddings entrenados de antemano, específicamente los creados por [\cite{MikolovetAl}].\\
 Dado el gran tamaño que tienen los archivos de los embeddings no fueron incluidos en el repositorio de GitHub. Cabe destacar que para el caso de los embeddings en ingles se encuentran en un formato específico asociado a la biblioteca de Python PyMagnitude \footnote{https://github.com/plasticityai/magnitude}.

\section{Aprendizaje de máquinas y redes neuronales}
En el capítulo anterior se explicó que uno de los enfoques para trabajar con las metáforas es el basado en corpus. Eso generalmente implica el uso de técnicas consideradas aprendizaje de máquinas, particularmente aprendizaje supervisado para producir modelos entrenados en una tarea determinada.
\subsection{Redes neuronales}
Entre los métodos de aprendizaje de máquinas es cada vez más común escuchar hablar de redes neuronales. Pero, ¿qué es una red neuronal? Respondiendo esa pregunta de la forma más sencilla, es una función con muchísimos parámetros.\\
Una definición más formal sería decir que una red neuronal es un conjunto de neuronas conectadas que transmiten información numérica para producir uno o más números de salida. Cada neurona recibe un valor y tras pasarlo por una función de activación antes de convertirse en la salida. \\
El algoritmo que entrena este tipo de modelos tiene como objetivo principal minimizar una función de pérdida determinada actualizando los pesos de cada neurona(propagación hacia atrás o back propagation en inglés).
\subsection{RNN}
Una red neuronal recurrente(RNN por sus siglas en inglés) se diferencia de una red neuronal común en que permite usar como entrada la salida de estados anteriores mientras mantiene estados ocultos. Generalmente son utilizadas para tareas de NLP porque ofrecen ventajas como la posibilidad de procesar entradas de cualquier tamaño. Otra de sus principales ventajas es que permite que se tome en cuenta información pasada en el momento de computar.\\
Entre sus principales desventajas están el aumento del tiempo de cómputo necesario para entrenar y la dificultad para acceder a información de hace mucho tiempo. Esta última es la causa del fenómeno conocido como desvanecimiento del gradiente.\footnote{https://mlearninglab.com/2018/05/06/problema-de-desvanecimiento-del-gradiente-vanishing-gradient-problem/}
\subsection{GRU}
Una red GRU(Gated Recurrent Unit) es un tipo de red neuronal diseñada específicamente para contrarrestar el problema del desvanecimiento del gradiente. Para ello utilizan lo que se conoce como la "puerta de actualización" y la "puerta de reseteo" que son dos vectores que deciden cuánta de la información recibida se volverá salida. Estos vectores son entrenables y pueden guardar información anterior con facilidad.\\
La puerta de actualización le permite al modelo saber cuánta información de pasos anteriores debe pasar a la salida mientras que la puerta de reseteo dicta cuánta de esa información debe ser olvidada. Internamente eso se logra combinando aplicaciones de la función sigmoidea y la función tanh. 
\subsection{LSTM}
Las redes LSTM(Long Short-Term Memory) son una generalización de GRU por lo que resuelven el mismo problema. En este caso se tienen tres puertas: "la puerta de olvidar", "la puerta de entrada", "la puerta de salida" y el estado de la unidad.\\
La puerta de olvidar decide que información olvidar y cuál conservar. Recibe dos entradas: el estado oculto anterior y la entrada actual. Para procesar las entradas les aplica la función sigmoidea, mientras más cercano a 1 sea el valor obtenido más se conserva y si es más cercano a 0 se descarta.\\
La puerta de entrada decide qué información importante debe agregar al estado de la celda en función de las entradas anteriores. Para actualizar el estado de la celda, se pasa el estado oculto anterior  y la entrada actual  a una función sigmoidea. Eso decide qué valores se actualizarán transformando los valores para que estén entre 0 y 1. 0 significa que no es importante y 1 significa que es importante. también se pasa el estado oculto y la entrada actual a la función tanh para reducir los valores entre -1 y 1 para ayudar a regular la red. Luego se multiplica la salida de tanh con la salida sigmoidea. La salida sigmoidea decidirá qué información es importante mantener de la salida tanh.\\
Con lo calculado previamente se tiene suficiente información para calcular el estado de la celda. Primero, el estado de la celda se multiplica puntualmente por el vector obtenido de la puerta de olvidar. Esto tiene la posibilidad de eliminar valores en el estado de la celda si se multiplica por valores cercanos a 0. Luego se toma la salida de la puerta de entrada y se hace una suma puntual que actualiza el estado de la celda a nuevos valores que la red neuronal considera relevantes. Eso devuelve el nuevo estado de la celda.\\
Por último está la puerta de salida. La puerta de salida decide cuál debería ser el siguiente estado oculto que contiene información sobre entradas anteriores. El estado oculto también se usa para predicciones. Primero, se pasa el estado oculto anterior y la entrada actual a una función sigmoidea. Luego se pasa el estado de celda recién modificado a la función tanh. Se multiplica la salida de tanh con la salida sigmoidea para decidir qué información debe llevar el estado oculto. La salida es el estado oculto. El nuevo estado de celda y el nuevo oculto se transfieren al siguiente paso de tiempo.\\
Este tipo de red es particularmente buena para trabajar con texto y secuencias, de ahí que fueran el seleccionados para la experimentación.